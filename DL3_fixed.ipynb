{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most people who bother with the matter at all would admit that the English language is in a bad way, but it is generally assumed that we cannot by conscious action do anything about it. Our civilization is decadent, and our language–so the argument runs–must inevitably share in the general collapse. It follows that any struggle against the abuse of language is a sentimental archaism, like preferring candles to electric light or hansom cabs to aeroplanes. Underneath this lies the half-conscious belief that language is a natural growth and not an instrument which we shape for our own purposes.\n",
      "\n",
      "Now, it is clear that the decline of a language must ultimately have political and economic causes: it is not due simply to the bad influence of this or that individual writer. But an effect can become a cause, reinforcing the original cause and producing the same effect in an intensified form, and so on indefinitely. A man may take to drink because he feels himself to be a failure, and then fail all the more completely because he drinks. It is rather the same thing that is happening to the English language. It becomes ugly and inaccurate because our thoughts are foolish, but the slovenliness of our language makes it easier for us to have foolish thoughts. The point is that the process is reversible. Modern English, especially written English, is full of bad habits which spread by imitation and which can be avoided if one is willing to take the necessary trouble. If one gets rid of these habits one can think more clearly, and to think clearly is a necessary first step towards political regeneration: so that the fight against bad English is not frivolous and is not the exclusive concern of professional writers. I will come back to this presently, and I hope that by that time the meaning of what I have said here will have become clearer. Meanwhile, here are five specimens of the English language as it is now habitually written.\n",
      "\n",
      "These five passages have not been picked out because they are especially bad–I could have quoted far worse if I had chosen–but because they illustrate various of the mental vices from which we now suffer. They are a little below the average, but are fairly representative samples. I number them so that I can refer back to them when necessary:\n",
      "\n",
      "(1) I am not, indeed, sure whether it is not true to say that the Milton who once seemed not unlike a seventeenth-century Shelley had not become, out of an experience ever more bitter in each year, more alien (sic) to the founder of that Jesuit sect which nothing could induce him to tolerate.\n",
      "\n",
      "--PROFESSOR HAROLD LASKI (Essay in FREEDOM OF EXPRESSION)\n",
      "\n",
      "(2) Above all, we cannot play ducks and drakes with a native battery of idioms which prescribes such egregious collocations of vocables as the Basic PUT UP WITH for TOLERATE or PUT AT A LOSS for BEWILDER.\n",
      "\n",
      "--PROFESSOR LANCELOT HOGBEN (INTERGLOSSA)\n",
      "\n",
      "(3) On the one side we have the free personality; by definition it is not neurotic, for it has neither conflict nor dream. Its desires, such as they are, are transparent, for they are just what institutional approval keeps in the forefront of consciousness; another institutional pattern would alter their number and intensity; there is little in them that is natural, irreducible, or culturally dangerous. But ON THE OTHER SIDE, the social bond itself is nothing but the mutual reflection of these self-secure integrities. Recall the definition of love. Is not this the very picture of a small academic? Where is there a place in this hall of mirrors for either personality or fraternity?\n",
      "\n",
      "--Essay on psychology in POLITICS (New York)\n",
      "\n",
      "(4) All the \"best people\" from the gentlemen's clubs, and all the frantic fascist captains, united in common hatred of Socialism and bestial horror of the rising tide of the mass revolutionary movement, have turned to acts of provocation, to foul incendiarism, to medieval legends of poisoned wells, to legalize their own destruction of proletarian organizations, and rouse the agitated petty-bourgeoisie to chauvinistic fervor on behalf of the fight against the revolutionary way out of the crisis.\n",
      "\n",
      "--Communist pamphlet\n",
      "\n",
      "(5) If a new spirit is to be infused into this old country, there is one thorny and contentious reform which must be tackled, and that is the humanization and galvanization of the B.B.C. Timidity here will bespeak canker and atrophy of the soul. The heart of Britain may lee sound and of strong beat, for instance, but the British lion's roar at present is like that of Bottom in Shakespeare's MIDSUMMER NIGHT'S DREAM–as gentle as any sucking dove. A virile new Britain cannot continue indefinitely to be traduced in the eyes, or rather ears, of the world by the effete languors of Langham Place, brazenly masquerading as \"standard English.\" When the Voice of Britain is heard at nine o'clock, better far and infinitely less ludicrous to hear aitches honestly dropped than the present priggish, inflated, inhibited, school-ma'am-ish arch braying of blameless bashful mewing maidens.\n",
      "\n",
      "--Letter in TRIBUNE\n",
      "\n",
      "Each of these passages has faults of its own, but quite apart from avoidable ugliness, two qualities are common to all of them. The first is staleness of imagery; the other is lack of precision. The writer either has a meaning and cannot express it, or he inadvertently says something else, or he is almost indifferent as to whether his words mean anything or not. This mixture of vagueness and sheer incompetence is the most marked characteristic of modern English prose, and especially of any kind of political writing. As soon as certain topics are raised, the concrete melts into the abstract and no one seems able to think of turns of speech that are not hackneyed: prose consists less and less of WORDS chosen for the sake of their meaning, and more and more of PHRASES tacked together like the sections of a prefabricated hen-house. I list below, with notes and examples, various of the tricks by means of which the work of prose-construction is habitually dodged:\n",
      "\n",
      "DYING METAPHORS. A newly-invented metaphor assists thought by evoking a visual image, while on the other hand a metaphor which is technically \"dead\" (e.g., IRON RESOLUTION) has in effect reverted to being an ordinary word and can generally be used without loss of vividness. But in between these two classes there is a huge dump of worn-out metaphors which have lost all evocative power and are merely used because they save people the trouble of inventing phrases for themselves. Examples are: RING THE CHANGES ON, TAKE UP THE CUDGELS FOR, TOE THE LINE, RIDE ROUGHSHOD OVER, STAND SHOULDER TO SHOULDER WITH, PLAY INTO THE HANDS OF, AN AXE TO GRIND, GRIST TO THE MILL, FISHING IN TROUBLED WATERS, ON THE ORDER OF THE DAY, ACHILLES' HEEL, SWAN SONG, HOTBED. Many of these are used without knowledge of their meaning (what is a \"rift,\" for instance?), and incompatible metaphors are frequently mixed, a sure sign that the writer is not interested in what he is saying. Some metaphors now current have been twisted out of their original meaning without those who use them even being aware of the fact. For example, TOE THE LINE is sometimes written TOW THE LINE. Another example is THE HAMMER AND THE ANVIL, now always used with the implication that the anvil gets the worst of it. In real life it is always the anvil that breaks the hammer, never the other way about: a writer who stopped to think what he was saying would be aware of this, and would avoid perverting the original phrase.\n",
      "\n",
      "OPERATORS, or VERBAL FALSE LIMBS. These save the trouble of picking out appropriate verbs and nouns, and at the same time pad each sentence with extra syllables which give it an appearance of symmetry. Characteristic phrases are: RENDER INOPERATIVE, MILITATE AGAINST, PROVE UNACCEPTABLE, MAKE CONTACT WITH, BE SUBJECTED TO, GIVE RISE TO, GIVE GROUNDS FOR, HAVING THE EFFECT OF, PLAY A LEADING PART (RÔLE) IN, MAKE ITSELF FELT, TAKE EFFECT, EXHIBIT A TENDENCY TO, SERVE THE PURPOSE OF, etc., etc. The keynote is the elimination of simple verbs. Instead of being a single word, such as BREAK, STOP, SPOIL, MEND, KILL, a verb becomes a PHRASE, made up of a noun or adjective tacked on to some general-purposes verb as PROVE, SERVE, FORM, PLAY, RENDER. In addition, the passive voice is wherever possible used in preference to the active, and noun constructions are used instead of gerunds (BY EXAMINATION OF instead of BY EXAMINING). The range of verbs is further cut down by means of the '-IZE' AND 'DE-' formations, and banal statements are given an appearance of profundity by means of the NOT 'UN-' formation. Simple conjunctions and prepositions are replaced by such phrases as WITH RESPECT TO, HAVING REGARD TO, THE FACT THAT, BY DINT OF, IN VIEW OF, IN THE INTERESTS OF, ON THE HYPOTHESIS THAT; and the ends of sentences are saved from anti-climax by such resounding commonplaces as GREATLY TO BE DESIRED, CANNOT BE LEFT OUT OF ACCOUNT, A DEVELOPMENT TO BE EXPECTED IN THE NEAR FUTURE, DESERVING OF SERIOUS CONSIDERATION, BROUGHT TO A SATISFACTORY CONCLUSION, and so on and so forth.\n",
      "\n",
      "PRETENTIOUS DICTION. Words like PHENOMENON, ELEMENT, INDIVIDUAL (as noun), OBJECTIVE, CATEGORICAL, EFFECTIVE, VIRTUAL, BASIS, PRIMARY, PROMOTE, CONSTITUTE, EXHIBIT, EXPLOIT, UTILIZE, ELIMINATE, LIQUIDATE, are used to dress up simple statements and give an air of scientific impartiality to biased judgments. Adjectives like EPOCH-MAKING, EPIC, HISTORIC, UNFORGETTABLE, TRIUMPHANT, AGE-OLD, INEVITABLE, INEXORABLE, VERITABLE, are used to dignify the sordid processes of international politics, while writing that aims at glorifying war usually takes on an archaic color, its characteristic words being: REALM, THRONE, CHARIOT, MAILED FIST, TRIDENT, SWORD, SHIELD, BUCKLER, BANNER, JACKBOOT, CLARION. Foreign words and expressions such as CUL DE SAC, ANCIEN RÉGIME, DEUS EX MACHINA, MUTATIS MUTANDIS, STATUS QUO, GLEICHSCHALTUNG, WELTANSCHAUUNG, are used to give an air of culture and elegance. Except for the useful abbreviations I.E., E.G., and ETC., there is no real need for any of the hundreds of foreign phrases now current in English. Bad writers, and especially scientific, political and sociological writers, are nearly always haunted by the notion that Latin or Greek words are grander than Saxon ones, and unnecessary words like EXPEDITE, AMELIORATE, PREDICT, EXTRANEOUS, DERACINATED, CLANDESTINE, SUB-AQUEOUS and hundreds of others constantly gain ground from their Anglo-Saxon opposite numbers. [Note 1, below] The jargon peculiar to Marxist writing (HYENA, HANGMAN, CANNIBAL, PETTY BOURGEOIS, THESE GENTRY, LACKEY, FLUNKEY, MAD DOG, WHITE GUARD, etc.) consists largely of words and phrases translated from Russian, German or French; but the normal way of coining a new word is to use a Latin or Greek root with the appropriate affix and, where necessary, the '-ize' formation. It is often easier to make up words of this kind (DE-REGIONALIZE, IMPERMISSIBLE, EXTRAMARITAL, NON-FRAGMENTARY and so forth) than to think up the English words that will cover one's meaning. The result, in general, is an increase in slovenliness and vagueness.\n",
      "\n",
      "[Note: 1. An interesting illustration of this is the way in which the English flower names which were in use till very recently are being ousted by Greek ones, SNAPDRAGON becoming ANTIRRHINUM, FORGET-ME-NOT becoming MYOSOTIS, etc. It is hard to see any practical reason for this change of fashion: it is probably due to an instinctive turning-away from the more homely word and a vague feeling that the Greek word is scientific. (Author's footnote.)]\n",
      "\n",
      "MEANINGLESS WORDS. In certain kinds of writing, particularly in art criticism and literary criticism, it is normal to come across long passages which are almost completely lacking in meaning. [Note, below] Words like ROMANTIC, PLASTIC, VALUES, HUMAN, DEAD, SENTIMENTAL, NATURAL, VITALITY, as used in art criticism, are strictly meaningless, in the sense that they not only do not point to any discoverable object, but are hardly even expected to do so by the reader. When one critic writes, \"The outstanding feature of Mr. X's work is its living quality,\" while another writes, \"The immediately striking thing about Mr. X's work is its peculiar deadness,\" the reader accepts this as a simple difference of opinion If words like BLACK and WHITE were involved, instead of the jargon words DEAD and LIVING, he would see at once that language was being used in an improper way. Many political words are similarly abused. The word FASCISM has now no meaning except in so far as it signifies \"something not desirable.\" The words DEMOCRACY, SOCIALISM, FREEDOM, PATRIOTIC, REALISTIC, JUSTICE, have each of them several different meanings which cannot be reconciled with one another. In the case of a word like DEMOCRACY, not only is there no agreed definition, but the attempt to make one is resisted from all sides. It is almost universally felt that when we call a country democratic we are praising it: consequently the defenders of every kind of régime claim that it is a democracy, and fear that they might have to stop using the word if it were tied down to any one meaning. Words of this kind are often used in a consciously dishonest way. That is, the person who uses them has his own private definition, but allows his hearer to think he means something quite different. Statements like MARSHAL PÉTAIN WAS A TRUE PATRIOT, THE SOVIET PRESS IS THE FREEST IN THE WORLD, THE CATHOLIC CHURCH IS OPPOSED TO PERSECUTION, are almost always made with intent to deceive. Other words used in variable meanings, in most cases more or less dishonestly, are: CLASS, TOTALITARIAN, SCIENCE, PROGRESSIVE, REACTIONARY BOURGEOIS, EQUALITY.\n",
      "\n",
      "[Note: Example: \"Comfort's catholicity of perception and image, strangely Whitmanesque in range, almost the exact opposite in aesthetic compulsion, continues to evoke that trembling atmospheric accumulative hinting at a cruel, an inexorably serene timelessness...Wrey Gardiner scores by aiming at simple bulls-eyes with precision. Only they are not so simple, and through this contented sadness runs more than the surface bittersweet of resignation.\" (POETRY QUARTERLY.) (Author's footnote.)]\n",
      "\n",
      "Now that I have made this catalogue of swindles and perversions, let me give another example of the kind of writing that they lead to. This time it must of its nature be an imaginary one. I am going to translate a passage of good English into modern English of the worst sort. Here is a well-known verse from ECCLESIASTES:\n",
      "\n",
      "I returned, and saw under the sun, that the race is not to the swift, nor the battle to the strong, neither yet bread to the wise, nor yet riches to men of understanding, nor yet favor to men of skill; but time and chance happeneth.\n",
      "\n",
      "Here it is in modern English:\n",
      "\n",
      "Objective consideration of contemporary phenomena compels the conclusion that success or failure in competitive activities exhibits no tendency to be commensurate with innate capacity, but that a considerable element of the unpredictable must invariably be taken into account.\n",
      "\n",
      "This is a parody, but not a very gross one. Exhibit (3), above, for instance, contains several patches of the same kind of English. It will be seen that I have not made a full translation. The beginning and ending of the sentence follow the original meaning fairly closely, but in the middle the concrete illustrations–race, battle, bread–dissolve into the vague phrase \"success or failure in competitive activities.\" This had to be so, because no modern writer of the kind I am discussing–no one capable of using phrases like \"objective consideration of contemporary phenomena\"–would ever tabulate his thoughts in that precise and detailed way. The whole tendency of modern prose is away from concreteness. Now analyze these two sentences a little more closely. The first contains 49 words but only 60 syllables, and all its words are those of everyday life. The second contains 38 words of 90 syllables: 18 of its words are from Latin roots, and one from Greek. The first sentence contains six vivid images, and only one phrase (\"time and chance\") that could be called vague. The second contains not a single fresh, arresting phrase, and in spite of its 90 syllables it gives only a shortened version of the meaning contained in the first. Yet without a doubt it is the second kind of sentence that is gaining ground in modern English. I do not want to exaggerate. This kind of writing is not yet universal, and outcrops of simplicity will occur here and there in the worst-written page. Still, if you or I were told to write a few lines on the uncertainty of human fortunes, we should probably come much nearer to my imaginary sentence than to the one from ECCLESIASTES.\n",
      "\n",
      "As I have tried to show, modern writing at its worst does not consist in picking out words for the sake of their meaning and inventing images in order to make the meaning clearer. It consists in gumming together long strips of words which have already been set in order by someone else, and making the results presentable by sheer humbug. The attraction of this way of writing, is that it is easy. It is easier–even quicker, once you have the habit–to say IN MY OPINION IT IS A NOT UNJUSTIFIABLE ASSUMPTION THAT than to say I THINK. If you use ready-made phrases, you not only don't have to hunt about for words; you also don't have to bother with the rhythms of your sentences, since these phrases are generally so arranged as to be more or less euphonious. When you are composing in a hurry–when you are dictating to a stenographer, for instance, or making a public speech–it is natural to fall into a pretentious, Latinized style. Tags like A CONSIDERATION WHICH WE SHOULD DO WELL TO BEAR IN MIND OR A CONCLUSION TO WHICH ALL OF US WOULD READILY ASSENT will save many a sentence from coming down with a bump. By using stale metaphors, similes and idioms, you save much mental effort at the cost of leaving your meaning vague, not only for your reader but for yourself. This is the significance of mixed metaphors. The sole aim of a metaphor is to call up a visual image. When these images clash–as in THE FASCIST OCTOPUS HAS SUNG ITS SWAN SONG, THE JACKBOOT IS THROWN INTO THE MELTING POT–it can be taken as certain that the writer is not seeing a mental image of the objects he is naming; in other words he is not really thinking. Look again at the examples I gave at the beginning of this essay. Professor Laski (1) uses five negatives in 53 words. One of these is superfluous, making nonsense of the whole passage, and in addition there is the slip ALIEN for akin, making further nonsense, and several avoidable pieces of clumsiness which increase the general vagueness. Professor Hogben (2) plays ducks and drakes with a battery which is able to write prescriptions, and, while disapproving of the everyday phrase PUT UP WITH, is unwilling to look EGREGIOUS up in the dictionary and see what it means. (3), if one takes an uncharitable attitude towards it, is simply meaningless: probably one could work out its intended meaning by reading the whole of the article in which it occurs. In (4), the writer knows more or less what he wants to say, but an accumulation of stale phrases chokes him like tea leaves blocking a sink. In (5), words and meaning have almost parted company. People who write in this manner usually have a general emotional meaning–they dislike one thing and want to express solidarity with another–but they are not interested in the detail of what they are saying. A scrupulous writer, in every sentence that he writes, will ask himself at least four questions, thus: What am I trying to say? What words will express it? What image or idiom will make it clearer? Is this image fresh enough to have an effect? And he will probably ask himself two more: Could I put it more shortly? Have I said anything that is avoidably ugly? But you are not obliged to go to all this trouble. You can shirk it by simply throwing your mind open and letting the ready-made phrases come crowding in. They will construct your sentences for you–even think your thoughts for you, to a certain extent-and at need they will perform the important service of partially concealing your meaning even from yourself. It is at this point that the special connection between politics and the debasement of language becomes clear.\n",
      "\n",
      "In our time it is broadly true that political writing is bad writing. Where it is not true, it will generally be found that the writer is some kind of rebel, expressing his private opinions and not a \"party line.\" Orthodoxy, of whatever color, seems to demand a lifeless, imitative style. The political dialects to be found in pamphlets, leading articles, manifestoes, White Papers and the speeches of under-secretaries do, of course, vary from party to party, but they are all alike in that one almost never finds in them a fresh, vivid, home-made turn of speech. When one watches some tired hack on the platform mechanically repeating the familiar phrases–BESTIAL ATROCITIES, IRON HEEL, BLOODSTAINED TYRANNY, FREE PEOPLES OF THE WORLD, STAND SHOULDER TO SHOULDER–one often has a curious feeling that one is not watching a live human being but some kind of dummy: a feeling which suddenly becomes stronger at moments when the light catches the speaker's spectacles and turns them into blank discs which seem to have no eyes behind them. And this is not altogether fanciful. A speaker who uses that kind of phraseology has gone some distance towards turning himself into a machine. The appropriate noises are coming out of his larynx, but his brain is not involved as it would be if he were choosing his words for himself. If the speech he is making is one that he is accustomed to make over and over again, he may be almost unconscious of what he is saying, as one is when one utters the responses in church. And this reduced state of consciousness, if not indispensable, is at any rate favorable to political conformity.\n",
      "\n",
      "In our time, political speech and writing are largely the defense of the indefensible. Things like the continuance of British rule in India, the Russian purges and deportations, the dropping of the atom bombs on Japan, can indeed be defended, but only by arguments which are too brutal for most people to face, and which do not square with the professed aims of political parties. Thus political language has to consist largely of euphemism, question-begging and sheer cloudy vagueness. Defenseless villages are bombarded from the air, the inhabitants driven out into the countryside, the cattle machine-gunned, the huts set on fire with incendiary bullets: this is called PACIFICATION. Millions of peasants are robbed of their farms and sent trudging along the roads with no more than they can carry: this is called TRANSFER OF POPULATION or RECTIFICATION OF FRONTIERS. People are imprisoned for years without trial, or shot in the back of the neck or sent to die of scurvy in Arctic lumber camps: this is called ELIMINATION OF UNRELIABLE ELEMENTS. Such phraseology is needed if one wants to name things without calling up mental pictures of them. Consider for instance some comfortable English professor defending Russian totalitarianism. He cannot say outright, \"I believe in killing off your opponents when you can get good results by doing so.\" Probably, therefore, he will say something like this:\n",
      "\n",
      "While freely conceding that the Soviet régime exhibits certain features which the humanitarian may be inclined to deplore, we must, I think, agree that a certain curtailment of the right to political opposition is an unavoidable concomitant of transitional periods, and that the rigors which the Russian people have been called upon to undergo have been amply justified in the sphere of concrete achievement.\n",
      "\n",
      "The inflated style is itself a kind of euphemism. A mass of Latin words falls upon the facts like soft snow, blurring the outlines and covering up all the details. The great enemy of clear language is insincerity. When there is a gap between one's real and one's declared aims, one turns, as it were instinctively, to long words and exhausted idioms, like a cuttlefish squirting out ink. In our age there is no such thing as \"keeping out of politics.\" All issues are political issues, and politics itself is a mass of lies, evasions, folly, hatred and schizophrenia. When the general atmosphere is bad, language must suffer. I should expect to find–this is a guess which I have not sufficient knowledge to verify–that the German, Russian and Italian languages have all deteriorated in the last ten or fifteen years as a result of dictatorship.\n",
      "\n",
      "But if thought corrupts language, language can also corrupt thought. A bad usage can spread by tradition and imitation, even among people who should and do know better. The debased language that I have been discussing is in some ways very convenient. Phrases like A NOT UNJUSTIFIABLE ASSUMPTION, LEAVES MUCH TO BE DESIRED, WOULD SERVE NO GOOD PURPOSE, A CONSIDERATION WHICH WE SHOULD DO WELL TO BEAR IN MIND, are a continuous temptation, a packet of aspirins always at one's elbow. Look back through this essay, and for certain you will find that I have again and again committed the very faults I am protesting against. By this morning's post I have received a pamphlet dealing with conditions in Germany. The author tells me that he \"felt impelled\" to write it. I open it at random, and here is almost the first sentence that I see: \"[The Allies] have an opportunity not only of achieving a radical transformation of Germany's social and political structure in such a way as to avoid a nationalistic reaction in Germany itself, but at the same time of laying the foundations of a cooperative and unified Europe.\" You see, he \"feels impelled\" to write–feels, presumably, that he has something new to say–and yet his words, like cavalry horses answering the bugle, group themselves automatically into the familiar dreary pattern. This invasion of one's mind by ready-made phrases (LAY THE FOUNDATIONS, ACHIEVE A RADICAL TRANSFORMATION) can only be prevented if one is constantly on guard against them, and every such phrase anesthetizes a portion of one's brain.\n",
      "\n",
      "I said earlier that the decadence of our language is probably curable. Those who deny this would argue, if they produced an argument at all, that language merely reflects existing social conditions, and that we cannot influence its development by any direct tinkering with words and constructions. So far as the general tone or spirit of a language goes, this may be true, but it is not true in detail. Silly words and expressions have often disappeared, not through any evolutionary process but owing to the conscious action of a minority. Two recent examples were EXPLORE EVERY AVENUE and LEAVE NO STONE UNTURNED, which were killed by the jeers of a few journalists. There is a long list of fly-blown metaphors which could similarly be got rid of if enough people would interest themselves in the job; and it should also be possible to laugh the NOT 'UN-' formation out of existence, [Note, below] to reduce the amount of Latin and Greek in the average sentence, to drive out foreign phrases and strayed scientific words, and, in general, to make pretentiousness unfashionable. But all these are minor points. The defense of the English language implies more than this, and perhaps it is best to start by saying what it does NOT imply.\n",
      "\n",
      "[Note: One can cure oneself of the NOT 'UN-' formation by memorizing this sentence: A NOT UNBLACK DOG WAS CHASING A NOT UNSMALL RABBIT ACROSS A NOT UNGREEN FIELD. (Author's footnote.)]\n",
      "\n",
      "To begin with, it has nothing to do with archaism, with the salvaging of obsolete words and turns of speech, or with the setting-up of a \"standard-English\" which must never be departed from. On the contrary, it is especially concerned with the scrapping of every word or idiom which has outworn its usefulness. It has nothing to do with correct grammar and syntax, which are of no importance so long as one makes one's meaning clear, or with the avoidance of Americanisms, or with having what is called a \"good prose style.\" On the other hand it is not concerned with fake simplicity and the attempt to make written English colloquial. Nor does it even imply in every case preferring the Saxon word to the Latin one, though it does imply using the fewest and shortest words that will cover one's meaning. What is above all needed is to let the meaning choose the word, and not the other way about. In prose, the worst thing one can do with words is to surrender them. When you think of a concrete object, you think wordlessly, and then, if you want to describe the thing you have been visualizing, you probably hunt about till you find the exact words that seem to fit it. When you think of something abstract you are more inclined to use words from the start, and unless you make a conscious effort to prevent it, the existing dialect will come rushing in and do the job for you, at the expense of blurring or even changing your meaning. Probably it is better to put off using words as long as possible and get one's meaning as clear as one can through pictures or sensations. Afterwards one can choose–not simply ACCEPT–the phrases that will best cover the meaning, and then switch round and decide what impressions one's words are likely to make on another person. This last effort of the mind cuts out all stale or mixed images, all prefabricated phrases, needless repetitions, and humbug and vagueness generally. But one can often be in doubt about the effect of a word or a phrase, and one needs rules that one can rely on when instinct fails. I think the following rules will cover most cases:\n",
      "\n",
      "(i) Never use a metaphor, simile or other figure of speech which you are used to seeing in print.\n",
      "\n",
      "(ii) Never use a long word where a short one will do.\n",
      "\n",
      "(iii) If it is possible to cut a word out, always cut it out.\n",
      "\n",
      "(iv) Never use the passive where you can use the active.\n",
      "\n",
      "(v) Never use a foreign phrase, a scientific word or a jargon word if you can think of an everyday English equivalent.\n",
      "\n",
      "(vi) Break any of these rules sooner than say anything barbarous.\n",
      "\n",
      "These rules sound elementary, and so they are, but they demand a deep change of attitude in anyone who has grown used to writing in the style now fashionable. One could keep all of them and still write bad English, but one could not write the kind of stuff that I quoted in these five specimens at the beginning of this article.\n",
      "\n",
      "I have not here been considering the literary use of language, but merely language as an instrument for expressing and not for concealing or preventing thought. Stuart Chase and others have come near to claiming that all abstract words are meaningless, and have used this as a pretext for advocating a kind of political quietism. Since you don't know what Fascism is, how can you struggle against Fascism? One need not swallow such absurdities as this, but one ought to recognize that the present political chaos is connected with the decay of language, and that one can probably bring about some improvement by starting at the verbal end. If you simplify your English, you are freed from the worst follies of orthodoxy. You cannot speak any of the necessary dialects, and when you make a stupid remark its stupidity will be obvious, even to yourself. Political language-and with variations this is true of all political parties, from Conservatives to Anarchists–is designed to make lies sound truthful and murder respectable, and to give an appearance of solidity to pure wind. One cannot change this all in a moment, but one can at least change one's own habits, and from time to time one can even, if one jeers loudly enough, send some worn-out and useless phrase–some JACKBOOT, ACHILLES' HEEL, HOTBED, MELTING POT, ACID TEST, VERITABLE INFERNO or other lump of verbal refuse–into the dustbin where it belongs.\n"
     ]
    }
   ],
   "source": [
    "#First of all, let's load the text\n",
    "data = open(\"orwell.txt\",'r', encoding=\"utf-8\")\n",
    "text = \"\"\n",
    "for i in data:\n",
    "    text += i\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "import keras.utils as ku \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "def dataset_preparation(data):\n",
    "    corpus = data.lower().split(\"\\n\")    \n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "    input_sequences = []\n",
    "    for line in corpus:\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            input_sequences.append(n_gram_sequence) \n",
    "    max_sequence_len = max([len(x) for x in input_sequences])\n",
    "    input_sequences = np.array(pad_sequences(input_sequences,   \n",
    "                          maxlen=max_sequence_len, padding='pre'))\n",
    "    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "    label = ku.to_categorical(label, num_classes=total_words)\n",
    "    return predictors, label, max_sequence_len, total_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(predictors, label, max_sequence_len, total_words):\n",
    "    input_len = max_sequence_len - 1\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(total_words, 10, input_length=input_len))\n",
    "    model.add(LSTM(150))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(total_words, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    model.fit(predictors, label, epochs=10, verbose=1)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(seed_text, next_words, max_sequence_len, model):\n",
    "    for j in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen= \n",
    "                             max_sequence_len-1, padding='pre')\n",
    "        predicted = model.predict_classes(token_list, verbose=0)\n",
    "  \n",
    "        output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \" + output_word\n",
    "    return seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, max_len, total_words = dataset_preparation(text)\n",
    "#new_model = create_model(X, Y, max_len, total_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(new_model)\n",
    "\n",
    "#text = generate_text(\"this is\", 10, 647, new_model)\n",
    "#print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "def create_model_modifiable(predictors, label, max_sequence_len, total_words, opt, lr_rate, ep, bs, two_layers):\n",
    "    input_len = max_sequence_len - 1\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(total_words, 10, input_length=input_len))\n",
    "    if two_layers:\n",
    "        model.add(LSTM(150,return_sequences=True))\n",
    "        model.add(LSTM(50))\n",
    "    else:\n",
    "        model.add(LSTM(150))\n",
    "\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(total_words, activation='softmax'))\n",
    "    \n",
    "    if opt == 0:\n",
    "        opti = optimizers.SGD(lr=lr_rate)\n",
    "    if opt == 1:\n",
    "        opti = optimizers.RMSprop(lr=lr_rate/10)    \n",
    "    if opt == 2:\n",
    "        opti = optimizers.Adagrad(lr=lr_rate) \n",
    "    if opt == 3:\n",
    "        opti = optimizers.Adadelta(lr=lr_rate)  \n",
    "    if opt == 4:\n",
    "        opti = optimizers.Adam(lr=lr_rate)    \n",
    "    if opt == 5:\n",
    "        opti = optimizers.Adamax(lr=lr_rate) \n",
    "    if opt == 6:\n",
    "        opti = optimizers.Nadam(lr=lr_rate)                \n",
    "        \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opti)\n",
    "    model.summary()\n",
    "    model.fit(predictors, label, epochs=ep, verbose=1, batch_size = bs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 646, 10)           16710     \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 646, 150)          96600     \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 50)                40200     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1671)              85221     \n",
      "=================================================================\n",
      "Total params: 238,731\n",
      "Trainable params: 238,731\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1\n",
      "5406/5406 [==============================] - 67s 12ms/step - loss: 8.9726\n"
     ]
    }
   ],
   "source": [
    "test_model = create_model_modifiable(X, Y, max_len, total_words, 4, 0.05, 1, 100, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5406/5406 [==============================] - 28s 5ms/step\n",
      "8.7301357151445\n"
     ]
    }
   ],
   "source": [
    "print(test_model.evaluate(X,Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4, 0.005, 10, 50, False], [4, 0.005, 10, 50, True], [4, 0.01, 10, 50, False], [4, 0.01, 10, 50, True], [4, 0.05, 10, 50, False], [4, 0.05, 10, 50, True], [4, 0.1, 10, 50, False], [4, 0.1, 10, 50, True]]\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "def add_grid(old_array,added_array):\n",
    "    result = []\n",
    "    for array in old_array:\n",
    "        for element in added_array:\n",
    "            temp = copy.copy(array)\n",
    "            temp.append(element)\n",
    "            result.append(temp)\n",
    "    return result\n",
    "op_array = [[4]]\n",
    "lr_array = [0.005, 0.01, 0.05,0.1]\n",
    "ep_array = [10]\n",
    "bs_array = [50]\n",
    "lay_array = [False, True]\n",
    "\n",
    "grid = add_grid(op_array, lr_array)\n",
    "grid = add_grid(grid, ep_array)\n",
    "grid = add_grid(grid, bs_array)\n",
    "grid = add_grid(grid, lay_array)\n",
    "print(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: [4, 0.005, 10, 50, False]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 646, 10)           16710     \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 150)               96600     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1671)              252321    \n",
      "=================================================================\n",
      "Total params: 365,631\n",
      "Trainable params: 365,631\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "5406/5406 [==============================] - 56s 10ms/step - loss: 6.6310\n",
      "Epoch 2/10\n",
      "5406/5406 [==============================] - 53s 10ms/step - loss: 6.2788\n",
      "Epoch 3/10\n",
      "5406/5406 [==============================] - 53s 10ms/step - loss: 6.0114\n",
      "Epoch 4/10\n",
      "5406/5406 [==============================] - 53s 10ms/step - loss: 5.7383\n",
      "Epoch 5/10\n",
      "5406/5406 [==============================] - 53s 10ms/step - loss: 5.5174\n",
      "Epoch 6/10\n",
      "5406/5406 [==============================] - 53s 10ms/step - loss: 5.2634\n",
      "Epoch 7/10\n",
      "5406/5406 [==============================] - 53s 10ms/step - loss: 4.9565\n",
      "Epoch 8/10\n",
      "5406/5406 [==============================] - 54s 10ms/step - loss: 4.9049\n",
      "Epoch 9/10\n",
      "5406/5406 [==============================] - 59s 11ms/step - loss: 5.8427\n",
      "Epoch 10/10\n",
      "5406/5406 [==============================] - 64s 12ms/step - loss: 5.1875\n",
      "This is a metaphor that of the jargon sentence of the other\n",
      "5406/5406 [==============================] - 19s 4ms/step\n",
      "Hyperparameters: [4, 0.005, 10, 50, True]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 646, 10)           16710     \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 646, 150)          96600     \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 50)                40200     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1671)              85221     \n",
      "=================================================================\n",
      "Total params: 238,731\n",
      "Trainable params: 238,731\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "5406/5406 [==============================] - 89s 17ms/step - loss: 6.9449\n",
      "Epoch 2/10\n",
      "5406/5406 [==============================] - 88s 16ms/step - loss: 6.5096\n",
      "Epoch 3/10\n",
      "5406/5406 [==============================] - 92s 17ms/step - loss: 6.4892\n",
      "Epoch 4/10\n",
      "5406/5406 [==============================] - 92s 17ms/step - loss: 6.4885\n",
      "Epoch 5/10\n",
      "5406/5406 [==============================] - 97s 18ms/step - loss: 6.4908\n",
      "Epoch 6/10\n",
      "5406/5406 [==============================] - 91s 17ms/step - loss: 6.4939\n",
      "Epoch 7/10\n",
      "5406/5406 [==============================] - 101s 19ms/step - loss: 6.4855\n",
      "Epoch 8/10\n",
      "5406/5406 [==============================] - 93s 17ms/step - loss: 6.4963\n",
      "Epoch 9/10\n",
      "5406/5406 [==============================] - 88s 16ms/step - loss: 6.4964\n",
      "Epoch 10/10\n",
      "5406/5406 [==============================] - 89s 16ms/step - loss: 6.4894\n",
      "This is the the the the the the the the the the\n",
      "5406/5406 [==============================] - 29s 5ms/step\n",
      "Hyperparameters: [4, 0.01, 10, 50, False]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 646, 10)           16710     \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 150)               96600     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1671)              252321    \n",
      "=================================================================\n",
      "Total params: 365,631\n",
      "Trainable params: 365,631\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "5406/5406 [==============================] - 58s 11ms/step - loss: 6.7136\n",
      "Epoch 2/10\n",
      "5406/5406 [==============================] - 56s 10ms/step - loss: 6.0932\n",
      "Epoch 3/10\n",
      "5406/5406 [==============================] - 53s 10ms/step - loss: 5.8656\n",
      "Epoch 4/10\n",
      "5406/5406 [==============================] - 54s 10ms/step - loss: 5.6205\n",
      "Epoch 5/10\n",
      "5406/5406 [==============================] - 53s 10ms/step - loss: 5.3167\n",
      "Epoch 6/10\n",
      "5406/5406 [==============================] - 54s 10ms/step - loss: 7.7261\n",
      "Epoch 7/10\n",
      "5406/5406 [==============================] - 54s 10ms/step - loss: 6.8255\n",
      "Epoch 8/10\n",
      "5406/5406 [==============================] - 54s 10ms/step - loss: 6.4469\n",
      "Epoch 9/10\n",
      "5406/5406 [==============================] - 53s 10ms/step - loss: 6.1505\n",
      "Epoch 10/10\n",
      "5406/5406 [==============================] - 56s 10ms/step - loss: 5.8861\n",
      "This is the facts of the familiar phrase if the exact phrase\n",
      "5406/5406 [==============================] - 19s 4ms/step\n",
      "Hyperparameters: [4, 0.01, 10, 50, True]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 646, 10)           16710     \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 646, 150)          96600     \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, 50)                40200     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1671)              85221     \n",
      "=================================================================\n",
      "Total params: 238,731\n",
      "Trainable params: 238,731\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "5406/5406 [==============================] - 94s 17ms/step - loss: 7.1306\n",
      "Epoch 2/10\n",
      "5406/5406 [==============================] - 93s 17ms/step - loss: 6.7210\n",
      "Epoch 3/10\n",
      "5406/5406 [==============================] - 95s 18ms/step - loss: 6.7182\n",
      "Epoch 4/10\n",
      "5406/5406 [==============================] - 96s 18ms/step - loss: 6.7152\n",
      "Epoch 5/10\n",
      "5406/5406 [==============================] - 97s 18ms/step - loss: 6.7272\n",
      "Epoch 6/10\n",
      "5406/5406 [==============================] - 98s 18ms/step - loss: 6.7179\n",
      "Epoch 7/10\n",
      "5406/5406 [==============================] - 96s 18ms/step - loss: 6.7397\n",
      "Epoch 8/10\n",
      "5406/5406 [==============================] - 97s 18ms/step - loss: 6.7298\n",
      "Epoch 9/10\n",
      "5406/5406 [==============================] - 96s 18ms/step - loss: 6.7327\n",
      "Epoch 10/10\n",
      "5406/5406 [==============================] - 89s 16ms/step - loss: 6.7208\n",
      "This is of of of of of of of of of of\n",
      "5406/5406 [==============================] - 28s 5ms/step\n",
      "Hyperparameters: [4, 0.05, 10, 50, False]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 646, 10)           16710     \n",
      "_________________________________________________________________\n",
      "lstm_13 (LSTM)               (None, 150)               96600     \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1671)              252321    \n",
      "=================================================================\n",
      "Total params: 365,631\n",
      "Trainable params: 365,631\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5406/5406 [==============================] - 56s 10ms/step - loss: 10.5497\n",
      "Epoch 2/10\n",
      "5406/5406 [==============================] - 53s 10ms/step - loss: 11.2479\n",
      "Epoch 3/10\n",
      "5406/5406 [==============================] - 53s 10ms/step - loss: 11.3019\n",
      "Epoch 4/10\n",
      "5406/5406 [==============================] - 54s 10ms/step - loss: 11.2374\n",
      "Epoch 5/10\n",
      "5406/5406 [==============================] - 53s 10ms/step - loss: 11.1853\n",
      "Epoch 6/10\n",
      "5406/5406 [==============================] - 53s 10ms/step - loss: 11.2148\n",
      "Epoch 7/10\n",
      "5406/5406 [==============================] - 53s 10ms/step - loss: 11.0842\n",
      "Epoch 8/10\n",
      "5406/5406 [==============================] - 53s 10ms/step - loss: 11.0948\n",
      "Epoch 9/10\n",
      "5406/5406 [==============================] - 53s 10ms/step - loss: 11.0147\n",
      "Epoch 10/10\n",
      "5406/5406 [==============================] - 54s 10ms/step - loss: 10.7564\n",
      "This is not word word if a word word if a word\n",
      "5406/5406 [==============================] - 19s 3ms/step\n",
      "Hyperparameters: [4, 0.05, 10, 50, True]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, 646, 10)           16710     \n",
      "_________________________________________________________________\n",
      "lstm_14 (LSTM)               (None, 646, 150)          96600     \n",
      "_________________________________________________________________\n",
      "lstm_15 (LSTM)               (None, 50)                40200     \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1671)              85221     \n",
      "=================================================================\n",
      "Total params: 238,731\n",
      "Trainable params: 238,731\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "5406/5406 [==============================] - 90s 17ms/step - loss: 7.8652\n",
      "Epoch 2/10\n",
      "5406/5406 [==============================] - 87s 16ms/step - loss: 7.5093\n",
      "Epoch 3/10\n",
      "5406/5406 [==============================] - 87s 16ms/step - loss: 7.5758\n",
      "Epoch 4/10\n",
      "5406/5406 [==============================] - 92s 17ms/step - loss: 7.5487\n",
      "Epoch 5/10\n",
      "5406/5406 [==============================] - 90s 17ms/step - loss: 7.4826\n",
      "Epoch 6/10\n",
      "5406/5406 [==============================] - 93s 17ms/step - loss: 7.5020\n",
      "Epoch 7/10\n",
      "5406/5406 [==============================] - 90s 17ms/step - loss: 7.4758\n",
      "Epoch 8/10\n",
      "5406/5406 [==============================] - 93s 17ms/step - loss: 7.4967\n",
      "Epoch 9/10\n",
      "5406/5406 [==============================] - 90s 17ms/step - loss: 7.4821\n",
      "Epoch 10/10\n",
      "5406/5406 [==============================] - 89s 16ms/step - loss: 7.4581\n",
      "This is the the the the the the the the the the\n",
      "5406/5406 [==============================] - 29s 5ms/step\n",
      "Hyperparameters: [4, 0.1, 10, 50, False]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 646, 10)           16710     \n",
      "_________________________________________________________________\n",
      "lstm_16 (LSTM)               (None, 150)               96600     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1671)              252321    \n",
      "=================================================================\n",
      "Total params: 365,631\n",
      "Trainable params: 365,631\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "5406/5406 [==============================] - 56s 10ms/step - loss: 14.7427\n",
      "Epoch 2/10\n",
      "5406/5406 [==============================] - 54s 10ms/step - loss: 15.2295\n",
      "Epoch 3/10\n",
      "5406/5406 [==============================] - 54s 10ms/step - loss: 15.2594\n",
      "Epoch 4/10\n",
      "5406/5406 [==============================] - 53s 10ms/step - loss: 15.2594\n",
      "Epoch 5/10\n",
      "5406/5406 [==============================] - 54s 10ms/step - loss: 15.2594\n",
      "Epoch 6/10\n",
      "5406/5406 [==============================] - 54s 10ms/step - loss: 15.2594\n",
      "Epoch 7/10\n",
      "5406/5406 [==============================] - 54s 10ms/step - loss: 15.2594\n",
      "Epoch 8/10\n",
      "5406/5406 [==============================] - 54s 10ms/step - loss: 15.2594\n",
      "Epoch 9/10\n",
      "5406/5406 [==============================] - 54s 10ms/step - loss: 15.2594\n",
      "Epoch 10/10\n",
      "5406/5406 [==============================] - 53s 10ms/step - loss: 15.2594\n",
      "This is the the the the the the the the the the\n",
      "5406/5406 [==============================] - 19s 4ms/step\n",
      "Hyperparameters: [4, 0.1, 10, 50, True]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, 646, 10)           16710     \n",
      "_________________________________________________________________\n",
      "lstm_17 (LSTM)               (None, 646, 150)          96600     \n",
      "_________________________________________________________________\n",
      "lstm_18 (LSTM)               (None, 50)                40200     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1671)              85221     \n",
      "=================================================================\n",
      "Total params: 238,731\n",
      "Trainable params: 238,731\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "5406/5406 [==============================] - 94s 17ms/step - loss: 10.2972\n",
      "Epoch 2/10\n",
      "5406/5406 [==============================] - 92s 17ms/step - loss: 10.3434\n",
      "Epoch 3/10\n",
      "5406/5406 [==============================] - 90s 17ms/step - loss: 10.4794\n",
      "Epoch 4/10\n",
      "5406/5406 [==============================] - 91s 17ms/step - loss: 10.5242\n",
      "Epoch 5/10\n",
      "5406/5406 [==============================] - 92s 17ms/step - loss: 10.5930\n",
      "Epoch 6/10\n",
      "5406/5406 [==============================] - 91s 17ms/step - loss: 10.5515\n",
      "Epoch 7/10\n",
      "5406/5406 [==============================] - 92s 17ms/step - loss: 10.7200\n",
      "Epoch 8/10\n",
      "5406/5406 [==============================] - 91s 17ms/step - loss: 10.7022\n",
      "Epoch 9/10\n",
      "5406/5406 [==============================] - 91s 17ms/step - loss: 10.6829\n",
      "Epoch 10/10\n",
      "5406/5406 [==============================] - 91s 17ms/step - loss: 10.7249\n",
      "This is of of of of of of of of of of\n",
      "5406/5406 [==============================] - 29s 5ms/step\n",
      "{(4, 0.005, 10, 50, False): 'This is a metaphor that of the jargon sentence of the other', (4, 0.005, 10, 50, True): 'This is the the the the the the the the the the', (4, 0.01, 10, 50, False): 'This is the facts of the familiar phrase if the exact phrase', (4, 0.01, 10, 50, True): 'This is of of of of of of of of of of', (4, 0.05, 10, 50, False): 'This is not word word if a word word if a word', (4, 0.05, 10, 50, True): 'This is the the the the the the the the the the', (4, 0.1, 10, 50, False): 'This is the the the the the the the the the the', (4, 0.1, 10, 50, True): 'This is of of of of of of of of of of'}\n",
      "{(4, 0.005, 10, 50, False): 4.526364133660546, (4, 0.005, 10, 50, True): 6.241635960647013, (4, 0.01, 10, 50, False): 5.225891332382896, (4, 0.01, 10, 50, True): 6.357905092391093, (4, 0.05, 10, 50, False): 10.37671241555618, (4, 0.05, 10, 50, True): 7.070716983514321, (4, 0.1, 10, 50, False): 15.259417467191401, (4, 0.1, 10, 50, True): 10.605668770750937}\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "texts = dict()\n",
    "losses = dict()\n",
    "for array in grid:\n",
    "    print(\"Hyperparameters: \" + str(array))\n",
    "    temp_model = create_model_modifiable(X, Y, max_len, total_words, array[0], array[1], array[2], array[3], array[4])\n",
    "    text = generate_text(\"This is\", 10, 647, temp_model)\n",
    "    print(text)\n",
    "    texts[tuple(array)] = text\n",
    "    losses[tuple(array)] = temp_model.evaluate(X,Y)\n",
    "print(texts)\n",
    "print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((4, 0.005, 10, 50, False), 4.526364133660546), ((4, 0.01, 10, 50, False), 5.225891332382896)]\n",
      "[(4, 0.005, 10, 50, False), (4, 0.01, 10, 50, False)]\n"
     ]
    }
   ],
   "source": [
    "good_losses = sorted(losses.items(), key=lambda kv: kv[1])\n",
    "good_losses = good_losses[:2]\n",
    "\n",
    "print(good_losses)\n",
    "new_grid = []\n",
    "for good_loss in good_losses:\n",
    "    new_grid.append(good_loss[0])\n",
    "print(new_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: (4, 0.005, 10, 50, False)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_13 (Embedding)     (None, 646, 10)           16710     \n",
      "_________________________________________________________________\n",
      "lstm_19 (LSTM)               (None, 150)               96600     \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1671)              252321    \n",
      "=================================================================\n",
      "Total params: 365,631\n",
      "Trainable params: 365,631\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "5406/5406 [==============================] - 63s 12ms/step - loss: 7.2728\n",
      "Epoch 2/50\n",
      "5406/5406 [==============================] - 56s 10ms/step - loss: 6.4417\n",
      "Epoch 3/50\n",
      "5406/5406 [==============================] - 58s 11ms/step - loss: 6.2879\n",
      "Epoch 4/50\n",
      "5406/5406 [==============================] - 58s 11ms/step - loss: 6.1749\n",
      "Epoch 5/50\n",
      "5406/5406 [==============================] - 59s 11ms/step - loss: 6.0579\n",
      "Epoch 6/50\n",
      "5406/5406 [==============================] - 58s 11ms/step - loss: 5.9503\n",
      "Epoch 7/50\n",
      "5406/5406 [==============================] - 58s 11ms/step - loss: 5.8451\n",
      "Epoch 8/50\n",
      "5406/5406 [==============================] - 58s 11ms/step - loss: 5.7308\n",
      "Epoch 9/50\n",
      "5406/5406 [==============================] - 57s 11ms/step - loss: 5.6001\n",
      "Epoch 10/50\n",
      "5406/5406 [==============================] - 57s 11ms/step - loss: 5.4483\n",
      "Epoch 11/50\n",
      "5406/5406 [==============================] - 58s 11ms/step - loss: 5.2704\n",
      "Epoch 12/50\n",
      "5406/5406 [==============================] - 58s 11ms/step - loss: 5.0769\n",
      "Epoch 13/50\n",
      "5406/5406 [==============================] - 58s 11ms/step - loss: 4.8534\n",
      "Epoch 14/50\n",
      "5406/5406 [==============================] - 57s 11ms/step - loss: 4.6107\n",
      "Epoch 15/50\n",
      "5406/5406 [==============================] - 57s 11ms/step - loss: 4.3491\n",
      "Epoch 16/50\n",
      "5406/5406 [==============================] - 57s 11ms/step - loss: 4.0556\n",
      "Epoch 17/50\n",
      "5406/5406 [==============================] - 57s 10ms/step - loss: 3.7275\n",
      "Epoch 18/50\n",
      "5406/5406 [==============================] - 57s 11ms/step - loss: 3.3962\n",
      "Epoch 19/50\n",
      "5406/5406 [==============================] - 56s 10ms/step - loss: 3.0380\n",
      "Epoch 20/50\n",
      "5406/5406 [==============================] - 57s 11ms/step - loss: 2.6771\n",
      "Epoch 21/50\n",
      "5406/5406 [==============================] - 57s 11ms/step - loss: 2.3114\n",
      "Epoch 22/50\n",
      "5406/5406 [==============================] - 57s 11ms/step - loss: 2.0058\n",
      "Epoch 23/50\n",
      "5406/5406 [==============================] - 57s 10ms/step - loss: 1.7191\n",
      "Epoch 24/50\n",
      "5406/5406 [==============================] - 57s 10ms/step - loss: 1.4586\n",
      "Epoch 25/50\n",
      "5406/5406 [==============================] - 56s 10ms/step - loss: 1.2599\n",
      "Epoch 26/50\n",
      "5406/5406 [==============================] - 57s 11ms/step - loss: 1.0965\n",
      "Epoch 27/50\n",
      "5406/5406 [==============================] - 57s 10ms/step - loss: 0.9572\n",
      "Epoch 28/50\n",
      "5406/5406 [==============================] - 57s 11ms/step - loss: 0.8107\n",
      "Epoch 29/50\n",
      "5406/5406 [==============================] - 57s 11ms/step - loss: 0.6856\n",
      "Epoch 30/50\n",
      "5406/5406 [==============================] - 57s 11ms/step - loss: 0.5741\n",
      "Epoch 31/50\n",
      "5406/5406 [==============================] - 56s 10ms/step - loss: 0.5244\n",
      "Epoch 32/50\n",
      "5406/5406 [==============================] - 57s 10ms/step - loss: 0.4405\n",
      "Epoch 33/50\n",
      "5406/5406 [==============================] - 58s 11ms/step - loss: 0.3542\n",
      "Epoch 34/50\n",
      "5406/5406 [==============================] - 57s 10ms/step - loss: 0.3024\n",
      "Epoch 35/50\n",
      "5406/5406 [==============================] - 57s 10ms/step - loss: 0.2383\n",
      "Epoch 36/50\n",
      "5406/5406 [==============================] - 57s 11ms/step - loss: 0.2194\n",
      "Epoch 37/50\n",
      "5406/5406 [==============================] - 56s 10ms/step - loss: 0.2011\n",
      "Epoch 38/50\n",
      "5406/5406 [==============================] - 57s 11ms/step - loss: 0.1881\n",
      "Epoch 39/50\n",
      "5406/5406 [==============================] - 57s 10ms/step - loss: 0.1581\n",
      "Epoch 40/50\n",
      "5406/5406 [==============================] - 57s 11ms/step - loss: 0.1307\n",
      "Epoch 41/50\n",
      "5406/5406 [==============================] - 57s 11ms/step - loss: 0.1143\n",
      "Epoch 42/50\n",
      "5406/5406 [==============================] - 57s 11ms/step - loss: 0.0946\n",
      "Epoch 43/50\n",
      "5406/5406 [==============================] - 57s 11ms/step - loss: 0.1038\n",
      "Epoch 44/50\n",
      "5406/5406 [==============================] - 57s 11ms/step - loss: 0.1407\n",
      "Epoch 45/50\n",
      "5406/5406 [==============================] - 57s 11ms/step - loss: 0.1135\n",
      "Epoch 46/50\n",
      "5406/5406 [==============================] - 58s 11ms/step - loss: 0.1029\n",
      "Epoch 47/50\n",
      "5406/5406 [==============================] - 57s 11ms/step - loss: 0.2073\n",
      "Epoch 48/50\n",
      "5406/5406 [==============================] - 57s 11ms/step - loss: 0.3241\n",
      "Epoch 49/50\n",
      "5406/5406 [==============================] - 57s 10ms/step - loss: 0.2032\n",
      "Epoch 50/50\n",
      "5406/5406 [==============================] - 57s 11ms/step - loss: 0.1419\n",
      "This is a parody but not a very gross one exhibit 3\n",
      "5406/5406 [==============================] - 22s 4ms/step\n",
      "Hyperparameters: (4, 0.01, 10, 50, False)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_14 (Embedding)     (None, 646, 10)           16710     \n",
      "_________________________________________________________________\n",
      "lstm_20 (LSTM)               (None, 150)               96600     \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1671)              252321    \n",
      "=================================================================\n",
      "Total params: 365,631\n",
      "Trainable params: 365,631\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "5406/5406 [==============================] - 57s 11ms/step - loss: 6.5722\n",
      "Epoch 2/50\n",
      "5406/5406 [==============================] - 56s 10ms/step - loss: 6.0446\n",
      "Epoch 3/50\n",
      "5406/5406 [==============================] - 55s 10ms/step - loss: 5.7740\n",
      "Epoch 4/50\n",
      "5406/5406 [==============================] - 55s 10ms/step - loss: 5.4879\n",
      "Epoch 5/50\n",
      "5406/5406 [==============================] - 55s 10ms/step - loss: 5.3377\n",
      "Epoch 6/50\n",
      "5406/5406 [==============================] - 55s 10ms/step - loss: 4.9428\n",
      "Epoch 7/50\n",
      "5406/5406 [==============================] - 55s 10ms/step - loss: 4.4946\n",
      "Epoch 8/50\n",
      "5406/5406 [==============================] - 55s 10ms/step - loss: 4.0129\n",
      "Epoch 9/50\n",
      "5406/5406 [==============================] - 56s 10ms/step - loss: 3.5481\n",
      "Epoch 10/50\n",
      "5406/5406 [==============================] - 55s 10ms/step - loss: 3.0544\n",
      "Epoch 11/50\n",
      "5406/5406 [==============================] - 54s 10ms/step - loss: 2.6802\n",
      "Epoch 12/50\n",
      "5406/5406 [==============================] - 55s 10ms/step - loss: 2.3426\n",
      "Epoch 13/50\n",
      "5406/5406 [==============================] - 54s 10ms/step - loss: 1.9853\n",
      "Epoch 14/50\n",
      "5406/5406 [==============================] - 54s 10ms/step - loss: 1.6919\n",
      "Epoch 15/50\n",
      "5406/5406 [==============================] - 54s 10ms/step - loss: 1.4831\n",
      "Epoch 16/50\n",
      "5406/5406 [==============================] - 54s 10ms/step - loss: 1.3567\n",
      "Epoch 17/50\n",
      "5406/5406 [==============================] - 55s 10ms/step - loss: 1.2305\n",
      "Epoch 18/50\n",
      "5406/5406 [==============================] - 54s 10ms/step - loss: 1.2119\n",
      "Epoch 19/50\n",
      "5406/5406 [==============================] - 55s 10ms/step - loss: 1.3560\n",
      "Epoch 20/50\n",
      "5406/5406 [==============================] - 54s 10ms/step - loss: 1.2577\n",
      "Epoch 21/50\n",
      "5406/5406 [==============================] - 54s 10ms/step - loss: 1.2037\n",
      "Epoch 22/50\n",
      "5406/5406 [==============================] - 54s 10ms/step - loss: 1.3039\n",
      "Epoch 23/50\n",
      "5406/5406 [==============================] - 53s 10ms/step - loss: nan\n",
      "Epoch 24/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5406/5406 [==============================] - 52s 10ms/step - loss: nan\n",
      "Epoch 25/50\n",
      "5406/5406 [==============================] - 53s 10ms/step - loss: nan\n",
      "Epoch 26/50\n",
      "5406/5406 [==============================] - 52s 10ms/step - loss: nan\n",
      "Epoch 27/50\n",
      "5406/5406 [==============================] - 53s 10ms/step - loss: nan\n",
      "Epoch 28/50\n",
      "5406/5406 [==============================] - 53s 10ms/step - loss: nan\n",
      "Epoch 29/50\n",
      "5406/5406 [==============================] - 53s 10ms/step - loss: nan\n",
      "Epoch 30/50\n",
      "5406/5406 [==============================] - 53s 10ms/step - loss: nan\n",
      "Epoch 31/50\n",
      "5406/5406 [==============================] - 52s 10ms/step - loss: nan\n",
      "Epoch 32/50\n",
      "5406/5406 [==============================] - 54s 10ms/step - loss: nan\n",
      "Epoch 33/50\n",
      "5406/5406 [==============================] - 53s 10ms/step - loss: nan\n",
      "Epoch 34/50\n",
      "5406/5406 [==============================] - 53s 10ms/step - loss: nan\n",
      "Epoch 35/50\n",
      "5406/5406 [==============================] - 53s 10ms/step - loss: nan\n",
      "Epoch 36/50\n",
      "5406/5406 [==============================] - 54s 10ms/step - loss: nan\n",
      "Epoch 37/50\n",
      "5406/5406 [==============================] - 53s 10ms/step - loss: nan\n",
      "Epoch 38/50\n",
      "5406/5406 [==============================] - 53s 10ms/step - loss: nan\n",
      "Epoch 39/50\n",
      "5406/5406 [==============================] - 53s 10ms/step - loss: nan\n",
      "Epoch 40/50\n",
      "5406/5406 [==============================] - 53s 10ms/step - loss: nan\n",
      "Epoch 41/50\n",
      "5406/5406 [==============================] - 54s 10ms/step - loss: nan\n",
      "Epoch 42/50\n",
      "5406/5406 [==============================] - 53s 10ms/step - loss: nan\n",
      "Epoch 43/50\n",
      "5406/5406 [==============================] - 53s 10ms/step - loss: nan\n",
      "Epoch 44/50\n",
      "5406/5406 [==============================] - 53s 10ms/step - loss: nan\n",
      "Epoch 45/50\n",
      "5406/5406 [==============================] - 53s 10ms/step - loss: nan\n",
      "Epoch 46/50\n",
      "5406/5406 [==============================] - 53s 10ms/step - loss: nan\n",
      "Epoch 47/50\n",
      "5406/5406 [==============================] - 53s 10ms/step - loss: nan\n",
      "Epoch 48/50\n",
      "5406/5406 [==============================] - 53s 10ms/step - loss: nan\n",
      "Epoch 49/50\n",
      "5406/5406 [==============================] - 53s 10ms/step - loss: nan\n",
      "Epoch 50/50\n",
      "5406/5406 [==============================] - 53s 10ms/step - loss: nan\n",
      "This is          \n",
      "5406/5406 [==============================] - 19s 3ms/step\n",
      "{(4, 0.005, 10, 50, False): 'This is a parody but not a very gross one exhibit 3', (4, 0.01, 10, 50, False): 'This is          '}\n",
      "{(4, 0.005, 10, 50, False): 0.04317722883391774, (4, 0.01, 10, 50, False): nan}\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "models = dict()\n",
    "best_texts = dict()\n",
    "best_losses = dict()\n",
    "for array in new_grid:\n",
    "    print(\"Hyperparameters: \" + str(array))\n",
    "    temp_model = create_model_modifiable(X, Y, max_len, total_words, array[0], array[1], 50, array[3], array[4])\n",
    "    text = generate_text(\"This is\", 10, 647, temp_model)\n",
    "    print(text)\n",
    "    best_texts[tuple(array)] = text\n",
    "    best_losses[tuple(array)] = temp_model.evaluate(X,Y)\n",
    "    models[tuple(array)] = copy.copy(temp_model)\n",
    "print(best_texts)\n",
    "print(best_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model has learning rate 0.005\n",
      "It has 1 layer\n",
      "Generating text:\n",
      "This is a parody but not a very gross one exhibit 3 above for instance contains several patches of the same kind\n",
      "I am not indeed sure whether it is not true to say that the milton who once seemed not unlike a seventeenth\n",
      "English language is in modern english conditions and it and neither conflict nor dream its desires such as they are used to\n",
      "Truth it is clear that the decline of a language must ultimately have political and economic causes it is not due\n",
      "Lies a parody but not a very gross one exhibit 3 above for instance contains several patches of the same kind\n",
      "This is a parody but not a very gross one exhibit 3 above for instance contains several patches of the same kind\n"
     ]
    }
   ],
   "source": [
    "#for key, value in good_losses.item\n",
    "\n",
    "best_losses = sorted(best_losses.items(), key=lambda kv: kv[1])\n",
    "traits = best_losses[0][0]\n",
    "best_model = models[traits]\n",
    "print(\"The best model has learning rate \" + str(traits[1]))\n",
    "if traits[4]:\n",
    "    print(\"It has 2 layers\")\n",
    "else:\n",
    "    print(\"It has 1 layer\")\n",
    "print(\"Generating text:\")\n",
    "print(generate_text(\"This is\", 20, 647, best_model))\n",
    "print(generate_text(\"I am\", 20, 647, best_model))\n",
    "print(generate_text(\"English language\", 20, 647, best_model))\n",
    "print(generate_text(\"Truth\", 20, 647, best_model))\n",
    "print(generate_text(\"Lies\", 20, 647, best_model))\n",
    "print(generate_text(\"This is\", 20, 647, best_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
